{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Indexing Documents with Min-Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('faq_database.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of questions and answers in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Problem description\\nInfrastructure created in AWS with CD-Deploy Action needs to be destroyed\\nSolution description\\nFrom local:\\nterraform init -backend-config=\"key=mlops-zoomcamp-prod.tfstate\" --reconfigure\\nterraform destroy --var-file vars/prod.tfvars\\nAdded by Erick Calderin',\n",
       " 'section': 'Module 6: Best practices',\n",
       " 'question': 'How to destroy infrastructure created via GitHub Actions',\n",
       " 'course': 'mlops-zoomcamp'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal search engine to return the most similar documents to the new query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mminsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_fields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword_fields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "A simple search index using TF-IDF and cosine similarity for text fields and exact matching for keyword fields.\n",
      "\n",
      "Attributes:\n",
      "    text_fields (list): List of text field names to index.\n",
      "    keyword_fields (list): List of keyword field names to index.\n",
      "    vectorizers (dict): Dictionary of TfidfVectorizer instances for each text field.\n",
      "    keyword_df (pd.DataFrame): DataFrame containing keyword field data.\n",
      "    text_matrices (dict): Dictionary of TF-IDF matrices for each text field.\n",
      "    docs (list): List of documents indexed.\n",
      "\u001b[0;31mInit docstring:\u001b[0m\n",
      "Initializes the Index with specified text and keyword fields.\n",
      "\n",
      "Args:\n",
      "    text_fields (list): List of text field names to index.\n",
      "    keyword_fields (list): List of keyword field names to index.\n",
      "    vectorizer_params (dict): Optional parameters to pass to TfidfVectorizer.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/Documents/Data-Science/LLM Courses/LLMZoomcamp/01-introduction/minsearch.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "minsearch.Index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"] # filter field to \"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x706ad67f7460>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboost_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Searches the index with the given query, filters, and boost parameters.\n",
      "\n",
      "Args:\n",
      "    query (str): The search query string.\n",
      "    filter_dict (dict): Dictionary of keyword fields to filter by. Keys are field names and values are the values to filter by.\n",
      "    boost_dict (dict): Dictionary of boost scores for text fields. Keys are field names and values are the boost scores.\n",
      "    num_results (int): The number of top results to return. Defaults to 10.\n",
      "\n",
      "Returns:\n",
      "    list of dict: List of documents matching the search criteria, ranked by relevance.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Documents/Data-Science/LLM Courses/LLMZoomcamp/01-introduction/minsearch.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "index.search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(question: str, filter_dict: dict = {}, num_results=5) -> list:\n",
    "    \"\"\"\n",
    "    Search for relevant results based on the given question and course.\n",
    "\n",
    "    Parameters:\n",
    "    question (str): The question to search for.\n",
    "    course (str): The course to filter the search results.\n",
    "    num_results (int): The number of results to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of search results.\n",
    "    \"\"\"\n",
    "    \n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=question,\n",
    "        filter_dict=filter_dict, # filter field to \"predict\"\n",
    "        boost_dict=boost,\n",
    "        num_results=num_results\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I still join the course after the start date?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - When will the course start?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - What can I do before the course starts?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I get support if I take the course in the self-paced mode?',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"The course has already started, can I still enroll?\"\n",
    "search(question=question, filter_dict={\"course\": \"data-engineering-zoomcamp\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have the list of the most similar answered-questions in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../.env\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Install the Google AI Python SDK\n",
    "\n",
    "$ pip install google-generativeai\n",
    "\n",
    "See the getting started guide for more information:\n",
    "https://ai.google.dev/gemini-api/docs/get-started/python\n",
    "\"\"\"\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "# See https://ai.google.dev/api/python/google/generativeai/GenerativeModel\n",
    "generation_config = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 1024,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    "    # safety_settings = Adjust safety settings\n",
    "    # See https://ai.google.dev/gemini-api/docs/safety-settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session = model.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last champion of the FIFA World Cup is **Argentina**, who won the tournament in **2022**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat_session.send_message(\"Who is the last champion of FIFA World Cup?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! But, asking about the specific Data Engineering Zoomcamp: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide me with more context! I need to know which course you're referring to in order to answer your question. \n",
      "\n",
      "Tell me:\n",
      "\n",
      "* **What is the name of the course?**\n",
      "* **Where is the course offered (online, university, etc.)?**\n",
      "\n",
      "Once I have this information, I can help you find out if you can still enroll. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat_session.send_message(question)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we do not get a good response. The general LLM model needs to know more the question context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM & RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query: str, search_results: list) -> str:\n",
    "    \"\"\"\n",
    "    Build a prompt for generating an answer based on a given user query and search results.\n",
    "\n",
    "    Args:\n",
    "        query (str): The question/query for which the prompt is being built.\n",
    "        search_results (list): A list of dictionaries containing search results.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prompt.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "If the CONTEXT is not enough to answer the QUESTION, return NONE.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_llm(prompt:str, model: genai.GenerativeModel, history: list = None) -> str:\n",
    "    \"\"\"\n",
    "    Invokes the LLM (Language Model) to generate a response based on the given prompt.\n",
    "\n",
    "    Parameters:\n",
    "    prompt (str): The input prompt for the LLM.\n",
    "    model (genai.GenerativeModel): The LLM model to be used for generating the response.\n",
    "    history (list, optional): A list of previous chat messages as history. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    str: The generated response from the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    \n",
    "    response = chat_session.send_message(prompt)\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_min_search(query: str, model: genai.GenerativeModel, filter_dict: dict = {}) -> str:\n",
    "    \"\"\"\n",
    "    Runs the RAG (Retrieval-Augmented Generation) model to generate an answer based on the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to search for.\n",
    "        model (genai.GenerativeModel): The RAG model to use for generation.\n",
    "        filter_dict (dict, optional): A dictionary of filters to apply during the search. Defaults to {}.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    search_results = search(query, filter_dict=filter_dict, num_results=5)\n",
    "\n",
    "    prompt = build_prompt(query, search_results)\n",
    "\n",
    "    answer = invoke_llm(prompt, model)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, even if you don't register, you're still eligible to submit the homeworks. \n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"The course has already started, can I still enroll?\"\n",
    "response = run_rag_min_search(query=question, model=model, filter_dict={\"course\": \"data-engineering-zoomcamp\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technically, yes. Advisable? Not really. Reasons:\n",
      "Some homework(s) asks for specific python library versions.\n",
      "Answers may not match in MCQ options if using different languages other than Python 3.10 (the recommended version for 2023 cohort)\n",
      "And as for midterms/capstones, your peer-reviewers may not know these other languages. Do you want to be penalized for others not knowing these other languages?\n",
      "You can create a separate repo using course’s lessons but written in other languages for your own learnings, but not advisable for submissions. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Can I use other programming languages in the course?\"\n",
    "response = run_rag_min_search(query=question, model=model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Elastic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-questions'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\"\n",
    "\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9a6a3127124be3b37a9cae8fdf66a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query: str, num_results:int = 5, filter_dict: dict ={}) -> list[str]:\n",
    "    \"\"\"\n",
    "    Perform an Elasticsearch search based on the given query and filter criteria.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "        num_results (int, optional): The number of results to return. Defaults to 5.\n",
    "        filter_dict (dict, optional): The filter criteria to apply to the search. Defaults to {}.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of search results as strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the search query\n",
    "    search_query = {\n",
    "        \"size\": num_results,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                        \"type\": \"best_fields\"\n",
    "                    }\n",
    "                },\n",
    "                \"filter\": {\n",
    "                    \"term\": filter_dict\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Perform the search using Elasticsearch client\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    # Extract the search results from the response\n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_elastic_search(query : str, model: genai.GenerativeModel, filter_dict: dict = {}):\n",
    "    \"\"\"\n",
    "    Runs the RAG (Retrieval-Augmented Generation) model on the given query using ElasticSearch.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        model (genai.GenerativeModel): The RAG model to be used for generation.\n",
    "        filter_dict (dict, optional): A dictionary of filters to be applied to the search results. Defaults to {}.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer based on the query and search results.\n",
    "    \"\"\"\n",
    "    \n",
    "    search_results = elastic_search(query, num_results=5, filter_dict=filter_dict)\n",
    "\n",
    "    prompt = build_prompt(query, search_results)\n",
    "\n",
    "    answer = invoke_llm(prompt, model)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, even if you don't register, you're still eligible to submit the homeworks. \n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"The course has already started, can I still enroll?\"\n",
    "response = run_rag_elastic_search(query=question, model=model, filter_dict={\"course\": \"data-engineering-zoomcamp\"})\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
